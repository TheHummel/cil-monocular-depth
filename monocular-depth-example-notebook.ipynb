{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96682,"databundleVersionId":11588651,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prep","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.130243Z","iopub.execute_input":"2025-04-04T12:37:37.130773Z","iopub.status.idle":"2025-04-04T12:37:37.136459Z","shell.execute_reply.started":"2025-04-04T12:37:37.130728Z","shell.execute_reply":"2025-04-04T12:37:37.135198Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data_dir = '/kaggle/input/ethz-cil-monocular-depth-estimation-2025'\ntrain_dir = os.path.join(data_dir, 'train/train')\ntest_dir = os.path.join(data_dir, 'test/test')\ntrain_list_file = os.path.join(data_dir, 'train_list.txt')\ntest_list_file = os.path.join(data_dir, 'test_list.txt')\noutput_dir = '/kaggle/working/'\nresults_dir = os.path.join(output_dir, 'results')\npredictions_dir = os.path.join(output_dir, 'predictions')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.137956Z","iopub.execute_input":"2025-04-04T12:37:37.138339Z","iopub.status.idle":"2025-04-04T12:37:37.158887Z","shell.execute_reply.started":"2025-04-04T12:37:37.138301Z","shell.execute_reply":"2025-04-04T12:37:37.157873Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 4\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-4\nNUM_EPOCHS = 1\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nINPUT_SIZE = (426, 560)\nNUM_WORKERS = 4\nPIN_MEMORY = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.160621Z","iopub.execute_input":"2025-04-04T12:37:37.160958Z","iopub.status.idle":"2025-04-04T12:37:37.182344Z","shell.execute_reply.started":"2025-04-04T12:37:37.160931Z","shell.execute_reply":"2025-04-04T12:37:37.181028Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Helper functions","metadata":{}},{"cell_type":"code","source":"def ensure_dir(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\ndef target_transform(depth):\n    # Resize the depth map to match input size\n    depth = torch.nn.functional.interpolate(\n        depth.unsqueeze(0).unsqueeze(0), \n        size=INPUT_SIZE, \n        mode='bilinear', \n        align_corners=True\n    ).squeeze()\n    \n    # Add channel dimension to match model output\n    depth = depth.unsqueeze(0)\n    return depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.183574Z","iopub.execute_input":"2025-04-04T12:37:37.183908Z","iopub.status.idle":"2025-04-04T12:37:37.202950Z","shell.execute_reply.started":"2025-04-04T12:37:37.183879Z","shell.execute_reply":"2025-04-04T12:37:37.201810Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class DepthDataset(Dataset):\n    def __init__(self, data_dir, list_file, transform=None, target_transform=None, has_gt=True):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.target_transform = target_transform\n        self.has_gt = has_gt\n        \n        # Read file list\n        with open(list_file, 'r') as f:\n            if has_gt:\n                self.file_pairs = [line.strip().split() for line in f]\n            else:\n                # For test set without ground truth\n                self.file_list = [line.strip() for line in f]\n    \n    def __len__(self):\n        return len(self.file_pairs if self.has_gt else self.file_list)\n    \n    def __getitem__(self, idx):\n        if self.has_gt:\n            rgb_path = os.path.join(self.data_dir, self.file_pairs[idx][0])\n            depth_path = os.path.join(self.data_dir, self.file_pairs[idx][1])\n            \n            # Load RGB image\n            rgb = Image.open(rgb_path).convert('RGB')\n            \n            # Load depth map\n            depth = np.load(depth_path).astype(np.float32)\n            depth = torch.from_numpy(depth)\n            \n            # Apply transformations\n            if self.transform:\n                rgb = self.transform(rgb)\n            \n            if self.target_transform:\n                depth = self.target_transform(depth)\n            else:\n                # Add channel dimension if not done by transform\n                depth = depth.unsqueeze(0)\n            \n            return rgb, depth, self.file_pairs[idx][0]  # Return filename for saving predictions\n        else:\n            # For test set without ground truth\n            rgb_path = os.path.join(self.data_dir, self.file_list[idx].split(' ')[0])\n            \n            # Load RGB image\n            rgb = Image.open(rgb_path).convert('RGB')\n            \n            # Apply transformations\n            if self.transform:\n                rgb = self.transform(rgb)\n            \n            return rgb, self.file_list[idx]  # No depth, just return the filename","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.203985Z","iopub.execute_input":"2025-04-04T12:37:37.204322Z","iopub.status.idle":"2025-04-04T12:37:37.217169Z","shell.execute_reply.started":"2025-04-04T12:37:37.204294Z","shell.execute_reply":"2025-04-04T12:37:37.216005Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Model - U-net","metadata":{}},{"cell_type":"code","source":"class UNetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UNetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.218287Z","iopub.execute_input":"2025-04-04T12:37:37.218658Z","iopub.status.idle":"2025-04-04T12:37:37.240868Z","shell.execute_reply.started":"2025-04-04T12:37:37.218621Z","shell.execute_reply":"2025-04-04T12:37:37.239793Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class SimpleUNet(nn.Module):\n    def __init__(self):\n        super(SimpleUNet, self).__init__()\n        \n        # Encoder blocks\n        self.enc1 = UNetBlock(3, 64)\n        self.enc2 = UNetBlock(64, 128)\n        \n        # Decoder blocks\n        self.dec2 = UNetBlock(128 + 64, 64)\n        self.dec1 = UNetBlock(64, 32)\n        \n        # Final layer\n        self.final = nn.Conv2d(32, 1, kernel_size=1)\n        \n        # Pooling and upsampling\n        self.pool = nn.MaxPool2d(2)\n        \n    def forward(self, x):\n        # Encoder\n        enc1 = self.enc1(x)\n        x = self.pool(enc1)\n        \n        x = self.enc2(x)\n        \n        # Decoder with skip connections\n        x = nn.functional.interpolate(x, size=enc1.shape[2:], mode='bilinear', align_corners=True)\n        x = torch.cat([x, enc1], dim=1)\n        x = self.dec2(x)\n        \n        x = self.dec1(x)\n        x = self.final(x)\n        \n        # Output non-negative depth values\n        x = torch.sigmoid(x)*10\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.242970Z","iopub.execute_input":"2025-04-04T12:37:37.243402Z","iopub.status.idle":"2025-04-04T12:37:37.270021Z","shell.execute_reply.started":"2025-04-04T12:37:37.243359Z","shell.execute_reply":"2025-04-04T12:37:37.269003Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Training loop","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n    \"\"\"Train the model and save the best based on validation metrics\"\"\"\n    best_val_loss = float('inf')\n    best_epoch = 0\n    train_losses = []\n    val_losses = []\n        \n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        # Training phase\n        model.train()\n        train_loss = 0.0\n        \n        for inputs, targets, _ in tqdm(train_loader, desc=\"Training\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * inputs.size(0)\n        \n        \n        train_loss /= len(train_loader.dataset)\n        train_losses.append(train_loss)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        \n        with torch.no_grad():\n            for inputs, targets, _ in tqdm(val_loader, desc=\"Validation\"):\n                inputs, targets = inputs.to(device), targets.to(device)\n                \n                # Forward pass\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                \n                val_loss += loss.item() * inputs.size(0)\n        \n        val_loss /= len(val_loader.dataset)\n        val_losses.append(val_loss)\n                \n        print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n        \n        # Save the best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_epoch = epoch\n            torch.save(model.state_dict(), os.path.join(results_dir, 'best_model.pth'))\n            print(f\"New best model saved at epoch {epoch+1} with validation loss: {val_loss:.4f}\")\n    \n    print(f\"\\nBest model was from epoch {best_epoch+1} with validation loss: {best_val_loss:.4f}\")\n    \n    # Load the best model\n    model.load_state_dict(torch.load(os.path.join(results_dir, 'best_model.pth')))\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.271699Z","iopub.execute_input":"2025-04-04T12:37:37.272094Z","iopub.status.idle":"2025-04-04T12:37:37.289651Z","shell.execute_reply.started":"2025-04-04T12:37:37.272030Z","shell.execute_reply":"2025-04-04T12:37:37.288563Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Model evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, val_loader, device):\n    \"\"\"Evaluate the model and compute metrics on validation set\"\"\"\n    model.eval()\n    \n    mae = 0.0\n    rmse = 0.0\n    rel = 0.0\n    delta1 = 0.0\n    delta2 = 0.0\n    delta3 = 0.0\n    sirmse = 0.0\n    \n    total_samples = 0\n    target_shape = None\n    \n    with torch.no_grad():\n        for inputs, targets, filenames in tqdm(val_loader, desc=\"Evaluating\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n            batch_size = inputs.size(0)\n            total_samples += batch_size\n            \n            if target_shape is None:\n                target_shape = targets.shape\n            \n\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Resize outputs to match target dimensions\n            outputs = nn.functional.interpolate(\n                outputs,\n                size=targets.shape[-2:],  # Match height and width of targets\n                mode='bilinear',\n                align_corners=True\n            )\n            \n            # Calculate metrics\n            abs_diff = torch.abs(outputs - targets)\n            mae += torch.sum(abs_diff).item()\n            rmse += torch.sum(torch.pow(abs_diff, 2)).item()\n            rel += torch.sum(abs_diff / (targets + 1e-6)).item()\n            \n            # Calculate scale-invariant RMSE for each image in the batch\n            for i in range(batch_size):\n                # Convert tensors to numpy arrays\n                pred_np = outputs[i].cpu().squeeze().numpy()\n                target_np = targets[i].cpu().squeeze().numpy()\n                \n                EPSILON = 1e-6\n                \n                valid_target = target_np > EPSILON\n                if not np.any(valid_target):\n                    continue\n                \n                target_valid = target_np[valid_target]\n                pred_valid = pred_np[valid_target]\n                \n                log_target = np.log(target_valid)\n                \n                pred_valid = np.where(pred_valid > EPSILON, pred_valid, EPSILON)\n                log_pred = np.log(pred_valid)\n                \n                # Calculate scale-invariant error\n                diff = log_pred - log_target\n                diff_mean = np.mean(diff)\n                \n                # Calculate RMSE for this image\n                sirmse += np.sqrt(np.mean((diff - diff_mean) ** 2))\n            \n            # Calculate thresholded accuracy\n            max_ratio = torch.max(outputs / (targets + 1e-6), targets / (outputs + 1e-6))\n            delta1 += torch.sum(max_ratio < 1.25).item()\n            delta2 += torch.sum(max_ratio < 1.25**2).item()\n            delta3 += torch.sum(max_ratio < 1.25**3).item()\n            \n            # Save some sample predictions\n            if total_samples <= 5 * batch_size:\n                for i in range(min(batch_size, 5)):\n                    idx = total_samples - batch_size + i\n                    \n                    # Convert tensors to numpy arrays\n                    input_np = inputs[i].cpu().permute(1, 2, 0).numpy()\n                    target_np = targets[i].cpu().squeeze().numpy()\n                    output_np = outputs[i].cpu().squeeze().numpy()\n                    \n                    # Normalize for visualization\n                    input_np = (input_np - input_np.min()) / (input_np.max() - input_np.min() + 1e-6)\n                    \n                    # Create visualization\n                    plt.figure(figsize=(15, 5))\n                    \n                    plt.subplot(1, 3, 1)\n                    plt.imshow(input_np)\n                    plt.title(\"RGB Input\")\n                    plt.axis('off')\n                    \n                    plt.subplot(1, 3, 2)\n                    plt.imshow(target_np, cmap='plasma')\n                    plt.title(\"Ground Truth Depth\")\n                    plt.axis('off')\n                    \n                    plt.subplot(1, 3, 3)\n                    plt.imshow(output_np, cmap='plasma')\n                    plt.title(\"Predicted Depth\")\n                    plt.axis('off')\n                    \n                    plt.tight_layout()\n                    plt.savefig(os.path.join(results_dir, f\"sample_{idx}.png\"))\n                    plt.close()\n            \n            # Free up memory\n            del inputs, targets, outputs, abs_diff, max_ratio\n            \n        # Clear CUDA cache\n        torch.cuda.empty_cache()\n    \n    # Calculate final metrics using stored target shape\n    total_pixels = target_shape[1] * target_shape[2] * target_shape[3]  # channels * height * width\n    mae /= total_samples * total_pixels\n    rmse = np.sqrt(rmse / (total_samples * total_pixels))\n    rel /= total_samples * total_pixels\n    sirmse = sirmse / total_samples\n    delta1 /= total_samples * total_pixels\n    delta2 /= total_samples * total_pixels\n    delta3 /= total_samples * total_pixels\n    \n    metrics = {\n        'MAE': mae,\n        'RMSE': rmse,\n        'siRMSE': sirmse,\n        'REL': rel,\n        'Delta1': delta1,\n        'Delta2': delta2,\n        'Delta3': delta3\n    }\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.290839Z","iopub.execute_input":"2025-04-04T12:37:37.291269Z","iopub.status.idle":"2025-04-04T12:37:37.312827Z","shell.execute_reply.started":"2025-04-04T12:37:37.291229Z","shell.execute_reply":"2025-04-04T12:37:37.311667Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Generate test predictions","metadata":{}},{"cell_type":"code","source":"def generate_test_predictions(model, test_loader, device):\n    \"\"\"Generate predictions for the test set without ground truth\"\"\"\n    model.eval()\n    \n    # Ensure predictions directory exists\n    ensure_dir(predictions_dir)\n    \n    with torch.no_grad():\n        for inputs, filenames in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n            inputs = inputs.to(device)\n            batch_size = inputs.size(0)\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Resize outputs to match original input dimensions (426x560)\n            outputs = nn.functional.interpolate(\n                outputs,\n                size=(426, 560),  # Original input dimensions\n                mode='bilinear',\n                align_corners=True\n            )\n            \n            # Save all test predictions\n            for i in range(batch_size):\n                # Get filename without extension\n                filename = filenames[i].split(' ')[1]\n                \n                # Save depth map prediction as numpy array\n                depth_pred = outputs[i].cpu().squeeze().numpy()\n                np.save(os.path.join(predictions_dir, f\"{filename}\"), depth_pred)\n            \n            # Clean up memory\n            del inputs, outputs\n        \n        # Clear cache after test predictions\n        torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.314252Z","iopub.execute_input":"2025-04-04T12:37:37.314679Z","iopub.status.idle":"2025-04-04T12:37:37.337126Z","shell.execute_reply.started":"2025-04-04T12:37:37.314605Z","shell.execute_reply":"2025-04-04T12:37:37.335926Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Putting it all together","metadata":{}},{"cell_type":"code","source":"def main():\n\n    # Create output directories\n    ensure_dir(results_dir)\n    ensure_dir(predictions_dir)\n    \n    # Define transforms\n    train_transform = transforms.Compose([\n        transforms.Resize(INPUT_SIZE),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Data augmentation\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    test_transform = transforms.Compose([\n        transforms.Resize(INPUT_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create training dataset with ground truth\n    train_full_dataset = DepthDataset(\n        data_dir=train_dir,\n        list_file=train_list_file, \n        transform=train_transform,\n        target_transform=target_transform,\n        has_gt=True\n    )\n    \n    # Create test dataset without ground truth\n    test_dataset = DepthDataset(\n        data_dir=test_dir,\n        list_file=test_list_file,\n        transform=test_transform,\n        has_gt=False  # Test set has no ground truth\n    )\n    \n    # Split training dataset into train and validation\n    total_size = len(train_full_dataset)\n    train_size = int(0.85 * total_size)  # 85% for training\n    val_size = total_size - train_size    # 15% for validation\n    \n    # Set a fixed random seed for reproducibility\n    torch.manual_seed(0)\n    \n    train_dataset, val_dataset = torch.utils.data.random_split(\n        train_full_dataset, [train_size, val_size]\n    )\n    \n    # Create data loaders with memory optimizations\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True, \n        num_workers=NUM_WORKERS, \n        pin_memory=PIN_MEMORY,\n        drop_last=True,\n        persistent_workers=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=False, \n        num_workers=NUM_WORKERS, \n        pin_memory=PIN_MEMORY\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=False, \n        num_workers=NUM_WORKERS, \n        pin_memory=PIN_MEMORY\n    )\n    \n    print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n    \n    # Clear CUDA cache before model initialization\n    torch.cuda.empty_cache()\n    \n    # Display GPU memory info\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n        print(f\"Initially allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n    \n    model = SimpleUNet()\n    model = nn.DataParallel(model)\n    model = model.to(DEVICE)\n    print(f\"Using device: {DEVICE}\")\n\n    # Print memory usage after model initialization\n    if torch.cuda.is_available():\n        print(f\"Memory allocated after model init: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n    \n    # Define loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    \n    \n    # Train the model\n    print(\"Starting training...\")\n    model = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, DEVICE)\n            \n    # Evaluate the model on validation set\n    print(\"Evaluating model on validation set...\")\n    metrics = evaluate_model(model, val_loader, DEVICE)\n    \n    # Print metrics\n    print(\"\\nValidation Metrics:\")\n    for name, value in metrics.items():\n        print(f\"{name}: {value:.4f}\")\n    \n    # Save metrics to file\n    with open(os.path.join(results_dir, 'validation_metrics.txt'), 'w') as f:\n        for name, value in metrics.items():\n            f.write(f\"{name}: {value:.4f}\\n\")\n    \n    # Generate predictions for the test set\n    print(\"Generating predictions for test set...\")\n    generate_test_predictions(model, test_loader, DEVICE)\n    \n    print(f\"Results saved to {results_dir}\")\n    print(f\"All test depth map predictions saved to {predictions_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.338192Z","iopub.execute_input":"2025-04-04T12:37:37.338630Z","iopub.status.idle":"2025-04-04T12:37:37.360909Z","shell.execute_reply.started":"2025-04-04T12:37:37.338586Z","shell.execute_reply":"2025-04-04T12:37:37.359891Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:37:37.363640Z","iopub.execute_input":"2025-04-04T12:37:37.364116Z","execution_failed":"2025-04-04T12:38:05.085Z"}},"outputs":[{"name":"stdout","text":"Train size: 20375, Validation size: 3596, Test size: 650\nUsing device: cpu\nStarting training...\nEpoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/5093 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Open a sample prediction from validation set\nImage.open('/kaggle/working/results/sample_0.png')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-04T12:38:05.085Z"}},"outputs":[],"execution_count":null}]}